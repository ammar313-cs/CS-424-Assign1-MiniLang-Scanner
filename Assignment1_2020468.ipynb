{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQRHoNZ3msMN",
        "outputId": "60d97310-b3b1-40e1-dd61-7484bcc285d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQbL04Sof5rX",
        "outputId": "f901591c-4890-46ae-f47e-8a70f8fd6859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the filename to scan: testCase3\n",
            "Lexical error in line 2: Invalid token\n",
            "('OPERATOR', '/', 1)\n",
            "('OPERATOR', '/', 1)\n",
            "('IDENTIFIER', 'MiniLang', 1)\n",
            "('IDENTIFIER', 'program', 1)\n",
            "('IDENTIFIER', 'with', 1)\n",
            "('IDENTIFIER', 'a', 1)\n",
            "('IDENTIFIER', 'mix', 1)\n",
            "('IDENTIFIER', 'of', 1)\n",
            "('IDENTIFIER', 'different', 1)\n",
            "('IDENTIFIER', 'token', 1)\n",
            "('IDENTIFIER', 'types', 1)\n",
            "('IDENTIFIER', 'x', 2)\n",
            "('OPERATOR', '=', 2)\n",
            "('INTEGER_LITERAL', '5', 2)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "TOKEN_TYPES = {\n",
        "    'INTEGER_LITERAL': r'\\d+',\n",
        "    'BOOLEAN_LITERAL': r'true|false',\n",
        "    'IDENTIFIER': r'[a-zA-Z][a-zA-Z0-9]*',\n",
        "    'OPERATOR': r'\\+|\\-|\\*|\\/|\\=|\\=\\=|\\!\\=',\n",
        "    'KEYWORD': r'if|else|print',\n",
        "    'COMMENT': r'\\/\\/.*',\n",
        "    'WHITESPACE': r'\\s+',\n",
        "}\n",
        "\n",
        "class Scanner:\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokens = []\n",
        "        self.keywords = ['if', 'else', 'print']\n",
        "        self.operators = ['+', '-', '*', '/', '=', '==', '!=']\n",
        "        self.current_line = 1\n",
        "\n",
        "    def scan(self):\n",
        "        with open(self.filename, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                while line:\n",
        "                    found_token = False\n",
        "                    for token_type, pattern in TOKEN_TYPES.items():\n",
        "                        match = re.match(pattern, line)\n",
        "                        if match:\n",
        "                            lexeme = match.group(0)\n",
        "                            if token_type != 'WHITESPACE':\n",
        "                                self.tokens.append((token_type, lexeme, self.current_line))\n",
        "                            line = line[match.end():].lstrip()\n",
        "                            found_token = True\n",
        "                            break\n",
        "\n",
        "                    if not found_token:\n",
        "                        print(f\"Lexical error in line {self.current_line}: Invalid token\")\n",
        "                        return\n",
        "\n",
        "                    if token_type == 'COMMENT':\n",
        "                        break\n",
        "                self.current_line += 1\n",
        "\n",
        "    def display_tokens(self):\n",
        "        for token in self.tokens:\n",
        "            print(token)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename = input(\"Enter the filename to scan: \")\n",
        "    scanner = Scanner(filename)\n",
        "    scanner.scan()\n",
        "    scanner.display_tokens()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a1X8qbZdmPax"
      }
    }
  ]
}